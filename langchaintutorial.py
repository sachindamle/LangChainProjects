# -*- coding: utf-8 -*-
"""LangChainTutorial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Yk2djzoPdkchhdf_GuLAbaFDHUA0Y14X
"""

!pip install openai

from google.colab import userdata
userdata.get('OPENAI_API_KEY')

import openai
import os
os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')

#!pip install
!pip install -qU langchain
!pip install -qU langchain-community
!pip install -qU pypdf
!pip install -qU chromadb
!pip install -qU tiktoken

from google.colab import drive
drive.mount('/content/drive')

path_to_pdf = !ls /content/drive/MyDrive/Learnings/ML4T\ Notes.pdf
path_to_pdf=path_to_pdf[0]
print(path_to_pdf)

import os
import openai
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.llms import OpenAI
from langchain.chains.question_answering import load_qa_chain
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.llms import AzureOpenAI
from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate

loader = PyPDFLoader("/content/drive/MyDrive/Learnings/ML4T Notes.pdf")

#Load the document by calling loader.load()
pages = loader.load()

print(len(pages))
print(pages[0].page_content[0:500])

print(pages[0].metadata)

rsplitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=10)
splits = rsplitter.split_documents(pages)
print(len(splits))
print(len(pages))

embeddings = OpenAIEmbeddings()

# Create the vector store
persist_directory = 'docs/chroma/'
vectorDB = Chroma.from_documents(documents=splits,
                                     embedding=embeddings,
                                     persist_directory=persist_directory)
print(vectorDB._collection.count())

model_name = "text-davinci-003"
model_name = "gpt-3.5-turbo"
llm = ChatOpenAI(model_name=model_name, temperature=0)

# Build prompt
template = """Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say "thanks for asking!" at the end of the answer.
{context}
Question: {question}
Helpful Answer:"""
QA_CHAIN_PROMPT = PromptTemplate.from_template(template)# Run chain
qaChain = RetrievalQA.from_chain_type(
        llm,
        retriever=vectorDB.as_retriever(),
        return_source_documents=True,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT}
    )

def AskMe(question):
  result = qaChain({"query": question})
  return result["result"]

# Pass question to the qa_chain
question = "What are major topics for this class?"
print(AskMe(question))

# Pass question to the qa_chain
question = "Summarize the document in 5000 words"
print(AskMe(question))

# Pass question to the qa_chain
question = "How machine learning can help trading?"
print(AskMe(question))

# Pass question to the qa_chain
question = "Who is Sachin Tendulkar?"
print(AskMe(question))